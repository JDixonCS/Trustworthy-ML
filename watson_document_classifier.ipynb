{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification and Attribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "To prepare your environment, you need to install some packages and enter credentials for the Watson services.\n",
    "\n",
    "### 1.1 Install the necessary packages\n",
    "\n",
    "You need the latest versions of these packages:<br>\n",
    "Watson Developer Cloud: a client library for Watson services.<br>\n",
    "NLTK: leading platform for building Python programs to work with human language data.<br>\n",
    "python-keystoneclient: is a client for the OpenStack Identity API.<br>\n",
    "python-swiftclient: is a python client for the Swift API.<br><br>\n",
    "** Install the Watson Developer Cloud package: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: watson-developer-cloud in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\n",
      "Requirement not upgraded as not directly required: pyOpenSSL>=16.2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: autobahn>=0.10.9 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: Twisted>=13.2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: python-dateutil>=2.5.3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: requests<3.0,>=2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: service-identity>=17.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: cryptography>=1.9 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: six>=1.5.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: txaio>=2.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from autobahn>=0.10.9->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: constantly>=15.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: zope.interface>=4.4.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: Automat>=0.3.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: hyperlink>=17.1.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: incremental>=16.10.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: attrs in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: pyasn1-modules in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: pyasn1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: asn1crypto>=0.21.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: cffi>=1.7 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: setuptools in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from zope.interface>=4.4.2->Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: pycparser in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cffi>=1.7->cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n"
     ]
    }
   ],
   "source": [
    "!pip install watson-developer-cloud==1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install NLTK: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\n",
      "Requirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from nltk)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install IBM Cloud Object Storage Client: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement not upgraded as not directly required: ibm-cos-sdk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\n",
      "Requirement not upgraded as not directly required: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: docutils>=0.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n"
     ]
    }
   ],
   "source": [
    "!pip install ibm-cos-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** <font color=blue>Now restart the kernel by choosing Kernel > Restart. </font> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import packages and libraries\n",
    "\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, EntitiesOptions, KeywordsOptions\n",
    "    \n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize,ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Add configurable items of the notebook below\n",
    "\n",
    "### 2.1 Add your service credentials from IBM Cloud for the Watson services\n",
    "\n",
    "You must create a Watson Natural Language Understanding service on IBM Cloud.\n",
    "Create a service for Natural Language Understanding (NLU).\n",
    "Insert the username and password values for your NLU in the following cell. Do not change the values of the version fields.\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2017-02-27',\n",
    "    username=\"\",\n",
    "    password=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Add your service credentials for Object Storage\n",
    "\n",
    "You must create Object Storage service on IBM Cloud.\n",
    "To access data in a file in Object Storage, you need the Object Storage authentication credentials.\n",
    "Insert the Object Storage authentication credentials as <i><b>credentials_1</b></i> in the following cell after \n",
    "removing the current contents in the cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials_1 = {\n",
    "    'IBM_API_KEY_ID': '',\n",
    "    'IAM_SERVICE_ID': '',\n",
    "    'ENDPOINT': '',\n",
    "    'IBM_AUTH_ENDPOINT': '',\n",
    "    'BUCKET': '',\n",
    "    'FILE': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Global Variables\n",
    "\n",
    "Add global variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file names for sample text and configuration files\n",
    "sampleTextFileName = \"sample_text.txt\"\n",
    "sampleConfigFileName = \"sample_config.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Configure and download required NLTK packages\n",
    "\n",
    "Download the 'punkt' and 'averaged_perceptron_tagger' NLTK packages for POS tagging usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dsxuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification\n",
    "\n",
    "Write the classification related utility functions in a modularized form.\n",
    "\n",
    "### 3.1 Watson NLU Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_using_NLU(analysistext):\n",
    "    \"\"\" Call Watson Natural Language Understanding service to obtain analysis results.\n",
    "    \"\"\"\n",
    "    response = natural_language_understanding.analyze( \n",
    "        text=analysistext,\n",
    "        features=Features(entities=EntitiesOptions(), \n",
    "                          keywords=KeywordsOptions()))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Augumented Classification\n",
    "\n",
    "Custom classification utlity fucntions for augumenting the results of Watson NLU API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\" Split text into sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "def keyword_tagging(tag,tagtext,text):\n",
    "    \"\"\" Tag the text matching keywords.\n",
    "    \"\"\"\n",
    "    if (text.lower().find(tagtext.lower()) != -1):\n",
    "        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "def regex_tagging(tag,regex,text):\n",
    "    \"\"\" Tag the text matching REGEX.\n",
    "    \"\"\"    \n",
    "    p = re.compile(regex, re.IGNORECASE)\n",
    "    matchtext = p.findall(text)\n",
    "    regex_list=[]    \n",
    "    if (len(matchtext)>0):\n",
    "        for regword in matchtext:\n",
    "            regex_list.append(regword)\n",
    "    return regex_list\n",
    "\n",
    "def chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    return chunk_list\n",
    "    \n",
    "def augument_NLUResponse(responsejson,updateType,text,tag):\n",
    "    \"\"\" Update the NLU response JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['keywords']):\n",
    "            responsejson['keywords'].append({\"text\":text,\"relevance\":0.5})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['entities']):\n",
    "            responsejson['entities'].append({\"type\":tag,\"text\":text,\"relevance\":0.5,\"count\":1})        \n",
    "    \n",
    "\n",
    "def classify_text(text, config):\n",
    "    \"\"\" Perform augumented classification of the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = analyze_using_NLU(text)\n",
    "    responsejson = response\n",
    "    \n",
    "    sentenceList = split_sentences(text)\n",
    "    \n",
    "    tokens = split_into_tokens(text)\n",
    "    \n",
    "    postags = POS_tagging(tokens)\n",
    "    \n",
    "    configjson = json.loads(config)\n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "        print('Stage - Performing ' + stages['name']+':')\n",
    "        for steps in stages['steps']:\n",
    "            print('    Step - ' + steps['type']+':')\n",
    "            if (steps['type'] == 'keywords'):\n",
    "                for keyword in steps['keywords']:\n",
    "                    for word in sentenceList:\n",
    "                        wordtag = keyword_tagging(keyword['tag'],keyword['text'],word)\n",
    "                        if(wordtag != 'UNKNOWN'):\n",
    "                            print('      '+keyword['tag']+':'+wordtag)\n",
    "                            augument_NLUResponse(responsejson,'entities',wordtag,keyword['tag'])\n",
    "            elif(steps['type'] == 'd_regex'):\n",
    "                for regex in steps['d_regex']:\n",
    "                    for word in sentenceList:\n",
    "                        regextags = regex_tagging(regex['tag'],regex['pattern'],word)\n",
    "                        if (len(regextags)>0):\n",
    "                            for words in regextags:\n",
    "                                print('      '+regex['tag']+':'+words)\n",
    "                                augument_NLUResponse(responsejson,'entities',words,regex['tag'])\n",
    "            elif(steps['type'] == 'chunking'):\n",
    "                for chunk in steps['chunk']:\n",
    "                    chunktags = chunk_tagging(chunk['tag'],chunk['pattern'],postags)\n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            print('      '+chunk['tag']+':'+words)\n",
    "                            augument_NLUResponse(responsejson,'entities',words,chunk['tag'])\n",
    "            else:\n",
    "                print('UNKNOWN STEP')\n",
    "    \n",
    "    return responsejson\n",
    "\n",
    "def replace_unicode_strings(response):\n",
    "    \"\"\" Convert dict with unicode strings to strings.\n",
    "    \"\"\"\n",
    "    if isinstance(response, dict):\n",
    "        return {replace_unicode_strings(key): replace_unicode_strings(value) for key, value in response.iteritems()}\n",
    "    elif isinstance(response, list):\n",
    "        return [replace_unicode_strings(element) for element in response]\n",
    "    elif isinstance(response, unicode):\n",
    "        return response.encode('utf-8')\n",
    "    else:\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Persistence and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 Configure Object Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.client('s3',\n",
    "                    ibm_api_key_id=credentials_1['IBM_API_KEY_ID'],\n",
    "                    ibm_service_instance_id=credentials_1['IAM_SERVICE_ID'],\n",
    "                    ibm_auth_endpoint=credentials_1['IBM_AUTH_ENDPOINT'],\n",
    "                    config=Config(signature_version='oauth'),\n",
    "                    endpoint_url=credentials_1['ENDPOINT'])\n",
    "\n",
    "def get_file(filename):\n",
    "    '''Retrieve file from Cloud Object Storage'''\n",
    "    fileobject = cos.get_object(Bucket=credentials_1['BUCKET'], Key=filename)['Body']\n",
    "    return fileobject\n",
    "\n",
    "def load_string(fileobject):\n",
    "    '''Load the file contents into a Python string'''\n",
    "    text = fileobject.read()\n",
    "    return text.decode('utf-8')\n",
    "\n",
    "def put_file(filename, filecontents):\n",
    "    '''Write file to Cloud Object Storage'''\n",
    "    resp = cos.put_object(Bucket=credentials_1['BUCKET'], Key=filename, Body=filecontents)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classify text\n",
    "Read the data file for classification from Object Store<br>\n",
    "Read the configuration file for augumented classification from Object Store.<br>\n",
    "Persist the classification results as JSON file in object store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Using the configuration ##\n",
      "{\n",
      "  \"configuration\": {\n",
      "    \"classification\": {\n",
      "      \"stages\": [\n",
      "        {\n",
      "          \"name\": \"Base Tagging\",\n",
      "          \"steps\": [\n",
      "            {\n",
      "              \"type\": \"keywords\",\n",
      "              \"keywords\": [\n",
      "                {\n",
      "                  \"tag\": \"Passion\",\n",
      "                  \"text\": \"Science\"\n",
      "                },\n",
      "                {\n",
      "                  \"tag\": \"Subjects\",\n",
      "                  \"text\": \"cosmology\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"d_regex\",\n",
      "              \"d_regex\": [\n",
      "                {\n",
      "                  \"tag\": \"Date\",\n",
      "                  \"pattern\": \"(\\\\d+/\\\\d+/\\\\d+)\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"d_regex\",\n",
      "              \"d_regex\": [\n",
      "                {\n",
      "                  \"tag\": \"Email\",\n",
      "                  \"pattern\": \"\\\\b[\\\\w.-]+?@\\\\w+?\\\\.\\\\w+?\\\\b\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"d_regex\",\n",
      "              \"d_regex\": [\n",
      "                {\n",
      "                  \"tag\": \"PhoneNumber\",\n",
      "                  \"pattern\": \"[0-9]{10}\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"chunking\",\n",
      "              \"chunk\": [\n",
      "                {\n",
      "                  \"tag\": \"NP\",\n",
      "                  \"pattern\": \"NP:{<DT>?<JJ>*<NN>}\"\n",
      "                },\n",
      "                {\n",
      "                  \"tag\": \"NAME\",\n",
      "                  \"pattern\": \"NAME:{<NNP>+}\"\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"Domain Tagging\",\n",
      "          \"steps\": [\n",
      "            {\n",
      "              \"type\": \"d_regex\",\n",
      "              \"d_regex\": [\n",
      "                {\n",
      "                  \"tag\": \"Year\",\n",
      "                  \"pattern\": \"[0-9]{4}\"\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trustworthy ML Rating: 90%\n",
    "# Load the text from Object Storage\n",
    "text = load_string(get_file(sampleTextFileName))\n",
    "\n",
    "# Load the json configuration from Object Storage\n",
    "config = load_string(get_file(sampleConfigFileName))\n",
    "\n",
    "# Print the json configuration\n",
    "print(\"## Using the configuration ##\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage - Performing Base Tagging:\n",
      "    Step - keywords:\n",
      "      Passion:science\n",
      "      Passion:science\n",
      "      Subjects:cosmology\n",
      "      Subjects:cosmology\n",
      "    Step - d_regex:\n",
      "      Date:01/08/1942\n",
      "    Step - d_regex:\n",
      "    Step - d_regex:\n",
      "      PhoneNumber:1112223333\n",
      "    Step - chunking:\n",
      "      NP: an early age\n",
      "      NP: a passion\n",
      "      NP: science\n",
      "      NP: the sky\n",
      "      NP: age\n",
      "      NP: cosmology\n",
      "      NP: amyotrophic lateral sclerosis\n",
      "      NP: illness\n",
      "      NP: work\n",
      "      NP: cosmology\n",
      "      NP: science\n",
      "      NP: everyone\n",
      "      NP: phone\n",
      "      NP: email\n",
      "      NP: yahoo.com\n",
      "      NAME: Stephen Hawking\n",
      "      NAME: Oxford\n",
      "      NAME: England\n",
      "      NAME: Hawking\n",
      "      NAME: University\n",
      "      NAME: Cambridge\n",
      "      NAME: Stephen Hawking\n",
      "      NAME: @\n",
      "Stage - Performing Domain Tagging:\n",
      "    Step - d_regex:\n",
      "      Year:1942\n",
      "      Year:1112\n",
      "      Year:2233\n"
     ]
    }
   ],
   "source": [
    "# Trustworthy ML Rating: 90%\n",
    "# Classify the text\n",
    "response = classify_text(text, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ Text Classification ~~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'entities': [{'count': 5,\n",
       "   'disambiguation': {'dbpedia_resource': 'http://dbpedia.org/resource/Stephen_Hawking',\n",
       "    'name': 'Stephen Hawking',\n",
       "    'subtype': ['Academic',\n",
       "     'Astronomer',\n",
       "     'AwardNominee',\n",
       "     'AwardWinner',\n",
       "     'BoardMember',\n",
       "     'Scientist',\n",
       "     'FilmActor',\n",
       "     'FilmWriter',\n",
       "     'TVActor']},\n",
       "   'relevance': 0.846941,\n",
       "   'text': 'Stephen Hawking',\n",
       "   'type': 'Person'},\n",
       "  {'count': 1,\n",
       "   'disambiguation': {'dbpedia_resource': 'http://dbpedia.org/resource/University_of_Cambridge',\n",
       "    'name': 'University of Cambridge',\n",
       "    'subtype': ['Location',\n",
       "     'CollegeUniversity',\n",
       "     'ProcessorManufacturer',\n",
       "     'University']},\n",
       "   'relevance': 0.202166,\n",
       "   'text': 'University of Cambridge',\n",
       "   'type': 'Organization'},\n",
       "  {'count': 1,\n",
       "   'disambiguation': {'dbpedia_resource': 'http://dbpedia.org/resource/Oxford',\n",
       "    'name': 'Oxford',\n",
       "    'subtype': ['AdministrativeDivision', 'PlaceWithNeighborhoods', 'City']},\n",
       "   'relevance': 0.200592,\n",
       "   'text': 'Oxford',\n",
       "   'type': 'Location'},\n",
       "  {'count': 1,\n",
       "   'disambiguation': {'dbpedia_resource': 'http://dbpedia.org/resource/England',\n",
       "    'name': 'England',\n",
       "    'subtype': ['PoliticalDistrict',\n",
       "     'AdministrativeDivision',\n",
       "     'GovernmentalJurisdiction',\n",
       "     'Country']},\n",
       "   'relevance': 0.180876,\n",
       "   'text': 'England',\n",
       "   'type': 'Location'},\n",
       "  {'count': 1,\n",
       "   'relevance': 0.180876,\n",
       "   'text': 'shawking@yahoo.com',\n",
       "   'type': 'EmailAddress'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': 'science', 'type': 'Passion'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': 'cosmology', 'type': 'Subjects'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': '01/08/1942', 'type': 'Date'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': '1112223333', 'type': 'PhoneNumber'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' an early age', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' a passion', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' science', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' the sky', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' age', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' cosmology', 'type': 'NP'},\n",
       "  {'count': 1,\n",
       "   'relevance': 0.5,\n",
       "   'text': ' amyotrophic lateral sclerosis',\n",
       "   'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' illness', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' work', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' everyone', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' phone', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' email', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' yahoo.com', 'type': 'NP'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' Stephen Hawking', 'type': 'NAME'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' Oxford', 'type': 'NAME'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' England', 'type': 'NAME'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' Hawking', 'type': 'NAME'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' University', 'type': 'NAME'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' Cambridge', 'type': 'NAME'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': ' @', 'type': 'NAME'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': '1942', 'type': 'Year'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': '1112', 'type': 'Year'},\n",
       "  {'count': 1, 'relevance': 0.5, 'text': '2233', 'type': 'Year'}],\n",
       " 'keywords': [{'relevance': 0.96866, 'text': 'Stephen Hawking'},\n",
       "  {'relevance': 0.884134, 'text': 'amyotrophic lateral sclerosis'},\n",
       "  {'relevance': 0.718936, 'text': 'debilitating illness'},\n",
       "  {'relevance': 0.66862, 'text': 'early age'},\n",
       "  {'relevance': 0.575803, 'text': 'cosmology'},\n",
       "  {'relevance': 0.513458, 'text': 'science'},\n",
       "  {'relevance': 0.492845, 'text': 'yahoo.com'},\n",
       "  {'relevance': 0.475153, 'text': 'passion'},\n",
       "  {'relevance': 0.464197, 'text': 'Oxford'},\n",
       "  {'relevance': 0.464044, 'text': 'England'},\n",
       "  {'relevance': 0.45856, 'text': 'sky'},\n",
       "  {'relevance': 0.4566, 'text': 'University'},\n",
       "  {'relevance': 0.456461, 'text': 'Cambridge'},\n",
       "  {'relevance': 0.453992, 'text': 'work'},\n",
       "  {'relevance': 0.45387, 'text': 'phone'},\n",
       "  {'relevance': 0.453855, 'text': 'physics'}],\n",
       " 'language': 'en',\n",
       " 'usage': {'features': 2, 'text_characters': 502, 'text_units': 1}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trustworthy ML Rating: 75%\n",
    "print(\"~~ Text Classification ~~\")\n",
    "\n",
    "# Store the classification response in Object Storage\n",
    "put_file(\"sample_text_classification.txt\", json.dumps(response))\n",
    "\n",
    "# Retrieve classification response from Object Storage\n",
    "json.loads(load_string(get_file(\"sample_text_classification.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
